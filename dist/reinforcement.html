<!DOCTYPE html><html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1"><link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet"><script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.5.0/font/bootstrap-icons.css"><style type="text/css">html {
	height: 100%;
	overflow-x: hidden;
}
.bg-color {
	background: #7CB9E8;
}
.btn-toolbar {
	height: 20px;
}

</style><script src="https://code.jquery.com/jquery-3.6.0.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.4.0/p5.min.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css" rel="stylesheet"><script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><script src="/static/scripts/reinforcement-game.js"></script><script src="/static/scripts/reinforcement-sketch.js"></script><title>Simulating Intelligence</title></head><body><div class="container-fluid d-flex flex-column" style="min-height:100vh"><div class="row bg-color p-5 text-center"><h1>Reinforcement Learning</h1></div><div class="row"><div class="col px-0"><nav class="navbar navbar-expand-md bg-dark navbar-dark"><div class="container-fluid px-2"><ul class="navbar-nav"><li class="nav-item"><a class="nav-link" href="/intro">Introduction</a></li><li class="navitem dropdown"><a class="nav-link dropdown-toggle" href="#" role="button" data-bs-toggle="dropdown">First Steps</a><ul class="dropdown-menu"><li><a class="dropdown-item" href="/algorithms">Algorithms</a></li><li><a class="dropdown-item" href="/heuristics">Heuristics</a></li></ul></li><li class="navitem dropdown"><a class="nav-link dropdown-toggle active" href="#" role="button" data-bs-toggle="dropdown">Learning</a><ul class="dropdown-menu"><li><a class="dropdown-item" href="/supervised-unsupervised">Supervised vs. Unsupervised</a></li><li><a class="dropdown-item" href="/reinforcement">Reinforcement Learning</a></li></ul></li><li class="navitem dropdown"><a class="nav-link dropdown-toggle" href="#" role="button" data-bs-toggle="dropdown">Neural Networks</a><ul class="dropdown-menu"><li><a class="dropdown-item" href="/perceptron">Perceptron</a></li><li><a class="dropdown-item" href="/feedforward">Feedforward NN</a></li><li><a class="dropdown-item" href="/convolutional">Convolutional NN</a></li><li><a class="dropdown-item" href="/object-detection">Object Detection</a></li></ul></li><li class="nav-item"><a class="nav-link" href="/genetic">Genetic Algorithms</a></li><li class="nav-item"><a class="nav-link" href="/language">Language</a></li><li class="nav-item"><a class="nav-link" href="/conclusion">Conclusion</a></li></ul></div></nav></div></div><div class="row p-0"><div class="col p-0"><div class="container mt-3"><p>In addition to supervised and unsupervised methods, computers can use reinforcement learning to learn a solution to a problem.  While both supervised and unsupervised learning are done by processing either labeled or unlabeled data, reinforcement learning is done by trial and error.  This tries to simulate how humans learn to perform tasks.</p><p>Imagine a baby learning to stand. It moves a certain combination of muscles and is probably able to rise slightly but eventually falls back down.  After that, it can try a slightly different combination of muscles which may get it closer to standing or may make it fall even quicker.  If the new combination of muscles gets it closer to standing, the baby can try similar combinations.  If not, the baby can try switching things up.</p><p>Reinforcement learning in computers works similarly.  When training, an agent creates a mapping between states and actions.  At certain points, it is given a reward based on its state.  The agent then uses the reward to update the mapping.  In the example of the baby, the baby is the agent.  The position of the baby (sitting/standing) is the state.  The actions are the muscles it can use to try to stand up.  After an attempt at standing, the reward is either positive if it succeeds or negative if it fails.  This is also identical to operant conditioning because the agent is rewarded to encourage behavior or punished to discourage it.</p><p>The simulation below shows a red dot in a grid.  The red dot is free to move either up, down, left, or right.  After the dot arrives at one of the green squares, it gets a reward of the corresponding number (\(5\), \(10\), or \(25\)) and the trial ends.  So, the red dot should be able to learn to go straight to the \(25\) point square every time in order to maximize the total reward.</p><p>To do this, the agent (red dot) needs to create a mapping between states (position) and actions (movements).  The numbers in all the gray squares are the agent’s expected reward from each square.  Before training, all of these numbers are \(0\), but they change as the agent explores the environment.  After each move, the new expected value is calculated and incorporated as a weighted average with the old value.  This new expected value, \(V\), is calculated using the Bellman equation: \[V(s) = \max_a{(R(s,a) + \gamma V(s’))}\] In this equation, \(s\) is the current state, \(a\) is a possible action from the current state, \(s’\) is the resulting state of action \(a\) on the current state, \(R(s,a)\) is the reward gained by taking action \(a\) from state \(s\), and \(\gamma\) is a constant called the discount factor.  This equation essentially means that the expected value is the maximum reward gained from moving to the next state plus the value of that state.  This is scaled slightly to prioritize rewards sooner rather than later, which incentivizes the agent to find a more direct path to rewards.</p><p>Now that the agent has a way of determining expected rewards from any state, it needs a way to choose an action from a state.  The agent’s optimal action can be found by calculating \[\DeclareMathOperator*{\argmax}{argmax}\argmax_a{V(s')}\] In other words, choose the action that leads to the state with the highest expected value.  However, if the agent always chooses the best possible action, it won’t be able to discover new solutions that could be better.  To solve this problem, the epsilon-greedy method is used, which tells the agent to go with its optimal solution most of the time but pick a random action some of the time.  Using a combination of the Bellman equation and the epsilon-greedy method, the agent is able to learn to get the reward of \(25\) if trained long enough.</p></div><div class="container-xl border p-3 mt-3"><div class="row pb-2"><div class="col fw-bold">Click the blue button to begin the learning process. Click it again to pause learning and show the optimal solution found so far.  The refresh button starts the simulation over.</div><div class="col-3 d-flex justify-content-center"><div class="btn-toolbar"><button class="btn btn-primary m-2" id="train-test" onclick="switch_mode()">Train</button><button class="btn btn-warning m-2 p-1" onclick="restart_sketch()"><i class="bi bi-arrow-repeat h4"></i></button></div></div></div><div class="row"><div class="d-flex justify-content-center" id="sketch"></div></div><div class="row"><a class="link-dark" href="#" data-bs-toggle="modal" data-bs-target="#code-view">View Code</a></div></div><div class="modal fade" id="code-view" aria-hidden="true"><div class="modal-dialog modal-xl"><div class="modal-content"><div class="modal-header"><h4 class="modal-title">Code</h4><button class="btn-close" type="button" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="accordion"><div class="accordion-item"><h2 class="accordion-header"><button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#script0">reinforcement-game.js</button></h2><div class="accordion-collapse collapse" id="script0"><div class="accordion-body"><pre><code id="code0"></code></pre></div></div></div><script>fetch("/static/scripts/reinforcement-game.js")
	.then(response => response.text())
	.then(text => $("#code0").text(text))
	.then(x => hljs.highlightElement($("#code0")[0]));
</script><div class="accordion-item"><h2 class="accordion-header"><button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#script1">reinforcement-sketch.js</button></h2><div class="accordion-collapse collapse" id="script1"><div class="accordion-body"><pre><code id="code1"></code></pre></div></div></div><script>fetch("/static/scripts/reinforcement-sketch.js")
	.then(response => response.text())
	.then(text => $("#code1").text(text))
	.then(x => hljs.highlightElement($("#code1")[0]));
</script></div></div><div class="modal-footer"><button class="btn btn-danger" type="button" data-bs-dismiss="modal">Close</button></div></div></div></div></div></div><div class="row align-items-end flex-fill"><div class="col"><div class="row mt-3"><div class="col-3 text-center"><a class="btn btn-outline-primary rounded-pill py-1 px-3" href="/supervised-unsupervised"><i class="bi bi-caret-left-fill"></i><span>Learning</span></a></div><div class="col-6"></div><div class="col-3 text-center"><a class="btn btn-outline-primary rounded-pill py-1 px-3" href="/perceptron"><span>Perceptron</span><i class="bi bi-caret-right-fill"></i></a></div></div><div class="row mt-3"><div class="col bg-color p-3"><span>Brady Bhalla, 2022</span></div></div></div></div></div></body></html>