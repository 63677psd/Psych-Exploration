extends /templates/simulation-template

block title
	- const title = "Reinforcement Learning"

block required_scripts
	- const required_scripts = ["reinforcement-game.js", "reinforcement-sketch.js"];

block nav_info
	- const current_page = "Learning";
	- const prev_page = {name: "Learning", link: "/supervised-unsupervised"};
	- const next_page = {name: "Perceptron", link: "/perceptron"};

block summary
	p In addition to supervised and unsupervised methods, computers can use reinforcement learning to learn a solution to a problem.  While both supervised and unsupervised learning are done by processing either labeled or unlabeled data, reinforcement learning is done by trial and error.  This tries to simulate how humans learn to perform tasks.
	p Imagine a baby learning to stand. It moves a certain combination of muscles and is probably able to rise slightly but eventually falls back down.  After that, it can try a slightly different combination of muscles which may get it closer to standing or may make it fall even quicker.  If the new combination of muscles gets it closer to standing, the baby can try similar combinations.  If not, the baby can try switching things up.
	p Reinforcement learning in computers works similarly.  When training, an agent creates a mapping between states and actions.  At certain points, it is given a reward based on its state.  The agent then uses the reward to update the mapping.  In the example of the baby, the baby is the agent.  The position of the baby (sitting/standing) is the state.  The actions are the muscles it can use to try to stand up.  After an attempt at standing, the reward is either positive if it succeeds or negative if it fails.  This is also identical to operant conditioning because the agent is rewarded to encourage behavior or punished to discourage it.
	p The simulation below shows a red dot in a grid.  The red dot is free to move either up, down, left, or right.  After the dot arrives at one of the green squares, it gets a reward of the corresponding number (\(5\), \(10\), or \(25\)) and the trial ends.  So, the red dot should be able to learn to go straight to the \(25\) point square every time in order to maximize the total reward.
	p To do this, the agent (red dot) needs to create a mapping between states (position) and actions (movements).  The numbers in all the gray squares are the agent’s expected reward from each square.  Before training, all of these numbers are \(0\), but they change as the agent explores the environment.  After each move, the new expected value is calculated and incorporated as a weighted average with the old value.  This new expected value, \(V\), is calculated using the Bellman equation: \[V(s) = \max_a{(R(s,a) + \gamma V(s’))}\] In this equation, \(s\) is the current state, \(a\) is a possible action from the current state, \(s’\) is the resulting state of action \(a\) on the current state, \(R(s,a)\) is the reward gained by taking action \(a\) from state \(s\), and \(\gamma\) is a constant called the discount factor.  This equation essentially means that the expected value is the maximum reward gained from moving to the next state plus the value of that state.  This is scaled slightly to prioritize rewards sooner rather than later, which incentivizes the agent to find a more direct path to rewards.
	p Now that the agent has a way of determining expected rewards from any state, it needs a way to choose an action from a state.  The agent’s optimal action can be found by calculating \[\DeclareMathOperator*{\argmax}{argmax}\argmax_a{V(s')}\] In other words, choose the action that leads to the state with the highest expected value.  However, if the agent always chooses the best possible action, it won’t be able to discover new solutions that could be better.  To solve this problem, the epsilon-greedy method is used, which tells the agent to go with its optimal solution most of the time but pick a random action some of the time.  Using a combination of the Bellman equation and the epsilon-greedy method, the agent is able to learn to get the reward of \(25\) if trained long enough.

block instructions
	.col.fw-bold Click the blue button to begin the learning process. Click it again to pause learning and show the optimal solution found so far.  The refresh button starts the simulation over.
	.col-3.d-flex.justify-content-center
		.btn-toolbar
			button.btn.btn-primary.m-2#train-test(onclick="switch_mode()") Train
			button.btn.btn-warning.m-2.p-1(onclick="restart_sketch()")
				i.bi.bi-arrow-repeat.h4