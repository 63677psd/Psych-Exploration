extends /templates/simulation-template

block title
	- const title = "Perceptron"

block required_scripts
	- const required_scripts = ["perceptron-sketch.js"];

block nav_info
	- const current_page = "Neural Networks";
	- const prev_page = {name: "Reinforcement Learning", link: "/reinforcement"};
	- const next_page = {name: "Feedforward NN", link: "/feedforward"};

block summary
	p This page and the next three pages will cover artificial neural networks, which attempt to model the human brain to create intelligent behavior.  Neural networks have led to some of the most advanced intelligent behavior demonstrated by computers.
	p The brain is made up of interconnected neurons, each of which takes inputs through its dendrites and either fires or doesnâ€™t fire an output signal through its axon.  The inputs to the neuron can be either inhibitory or excitatory.  The inhibitory inputs try to inhibit the neuron from firing while the excitatory inputs try to cause the neuron to fire.  If the strength of the excitatory inputs exceeds the strength of the inhibitory inputs, the neuron will fire.
	p A computer model of a single neuron, called a perceptron, functions similarly.  Each input, \(x_i\), to the perceptron has a certain weight, \(w_i\), which is the amount of impact the input will have on the perceptron.  Positive weights increase the output of the perceptron while negative weights decrease its output.  Each perceptron also has a bias, \(b\), which is its predisposition to firing without any inputs.  A positive bias increases the perceptron output while a negative bias decreases it.  As an expression, the output of the perceptron is \[\sigma(b + \sum_i{w_i x_i})\] This means that to find the output of a perceptron, multiply each input by its weight and total the results.  Then, add the bias and pass this value to another function, \(\sigma\), which is called the activation function and is used to incorporate nonlinearity into the perceptron.  Later, when many perceptrons are combined into a network, this nonlinearity will be what allows robust models to be created.  Different activation functions can be used, but the one in the simulation is called the sigmoid function, which is defined as \[\sigma(x) = \frac{1}{1 + e^{-x}}\] Biological neurons can either fire or do nothing, there is no in between.  Perceptrons, however, can output any value in the range of the activation function.  The sigmoid function is a popular activation function because it maps negative values to numbers close to \(0\) and positive values to numbers close to \(1\), which for numbers of large enough magnitude is essentially the same as firing or not firing.
	p In the simulation below, lighter colored circles have a value close to \(0\) and darker colored circles have a value close to \(1\).  Inputs are on the left and the perceptron output is on the right. The weights are the arrows that connect circles and the bias is the arrow pointing down.  Green arrows are positive and red arrows are negative.  The magnitude of the weights/bias corresponds to the thickness of the arrow.


block instructions
	.col.fw-bold Move the mouse around the simulation to randomly generate new input values for the perceptron. Clicking the refresh button will randomize the weights and bias for the perceptron.
	.col-3.d-flex.justify-content-center
		.btn-toolbar
			button.btn.btn-warning.m-2.p-1(onclick="restart_sketch()")
				i.bi.bi-arrow-repeat.h4